{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[2 9]\n",
      " [1 5]\n",
      " [3 6]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89802923]\n",
      " [0.88087037]\n",
      " [0.88904191]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(([2, 9], [1, 5], [3, 6])) # Hours Studied, Hours Slept \n",
    "y = np.array(([92], [86], [89])) # Test Score\n",
    "\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid(x): #this function maps any value between 0 and 1\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "epoch=10000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = 2 #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons of output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bias_hidden=np.random.uniform(size=(1,hiddenlayer_neurons)) #bias matrix to the hidden layer\n",
    "weight_hidden=np.random.uniform(size=(hiddenlayer_neurons,output_neurons)) #weight matrix to the output layer\n",
    "bias_output=np.random.uniform(size=(1,output_neurons)) # matrix to the output layer\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hinp1=np.dot(X,wh)\n",
    "    hinp= hinp1 + bias_hidden #bias_hidden GRADIENT DISCENT\n",
    "    hlayer_activation = sigmoid(hinp)\n",
    "    \n",
    "    outinp1=np.dot(hlayer_activation,weight_hidden)\n",
    "    outinp= outinp1+ bias_output\n",
    "    output = sigmoid(outinp)\n",
    "    \n",
    "    #Backpropagation\n",
    "    EO = y-output #Compare prediction with actual output and calculate the gradient of error (Actual â€“ Predicted)\n",
    "\n",
    "    outgrad = derivatives_sigmoid(output) #Compute the slope/ gradient of hidden and output layer neurons\n",
    "\n",
    "    d_output = EO * outgrad #Compute change factor(delta) at output layer, dependent on the gradient of error multiplied by the slope of output layer activation\n",
    "\n",
    "    EH = d_output.dot(weight_hidden.T)  #At this step, the error will propagate back into the network which means error at hidden layer. we will take the dot product of output layer delta with weight parameters of edges between the hidden and output layer (weight_hidden.T).\n",
    "\n",
    "    hiddengrad = derivatives_sigmoid(hlayer_activation) #how much hidden layer weight contributed to error\n",
    "    d_hiddenlayer = EH * hiddengrad\n",
    "\n",
    "\n",
    "    #update the weights\n",
    "    weight_hidden += hlayer_activation.T.dot(d_output) *lr# dot product of nextlayererror and currentlayerop\n",
    "    bias_hidden += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bias_output += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(\"Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" ,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
